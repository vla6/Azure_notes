{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a modified version of https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb\n",
    "Valerie Carey modified this script to demonstrate issues with Azure when Parquet files are used in a ParallelRunStep\n",
    "\n",
    "Original Link Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "Licensed under the MIT License.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Batch Inference Parquet Issues - Summary </h1>\n",
    "\n",
    "Modified version of https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb\n",
    "\n",
    "Here I show that an input dataset based on a Parquet file is NOT split up for batching, making a parallel run for that dataset type nonsensical.\n",
    "\n",
    "This is a problem for large jobs where you want to batch. I was trying over and over with different mini batch sizes and run invocation imeouts, and with Parquet I always get 1 mini batch (on the experiment summary page), regardless of file size.  However I see multiple processes try to run, presumably because the first won't finish.  I was doing Shapley explanations and so most often I would time out and my experiment would fail.  Shapley explanations are a prime candidate for parallelization because it is time consuming.\n",
    "\n",
    "I found this issue on Stack Overflow where the response was that you have to split your data into files: https://stackoverflow.com/questions/64869372/azure-ml-python-sdk-mini-batch-size-not-working-as-expected-on-parallelrunconfig\\\n",
    "However this is NOT true for CSV datasets which are split up just fine.  And, official Microsoft documentation implies parquet files are supported, for instance https://techcommunity.microsoft.com/t5/azure-ai/batch-inference-in-azure-machine-learning/ba-p/1417010\n",
    "\n",
    "The documentation for parallel running does not make it clear that it doesn't work for Parquet.  For example:\n",
    "* https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallel_run_config.parallelrunconfig?view=azure-ml-py\n",
    "* https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallelrunstep?view=azure-ml-py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Prerequisites </h4>\n",
    "\n",
    "I have deleted the parts of the example code that connect to a workspace, get blob storage, create a compute instance, etc.  I assume you have these resources already.  You can modify the code below to connect to your own resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Connect to workspace </h2>\n",
    "Modify the code below to connect to your own workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.20.0\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_NAME = 'YOUR-INFO-HERE'\n",
    "WORKSPACE_SUBSCRIPTION_ID = \"YOUR-INFO-HERE\"\n",
    "WORKSPACE_RESOURCE_GROUP = \"YOUR-INFO-HERE\"\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.get(name=WORKSPACE_NAME,\n",
    "               subscription_id=WORKSPACE_SUBSCRIPTION_ID,\n",
    "               resource_group=WORKSPACE_RESOURCE_GROUP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Set your datastore and compute </h2>\n",
    "Modify the code below to specify your own datastore and blob storage locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "# input datastore\n",
    "iris_data = Datastore.get(ws, 'YOUR-INFO-HERE')\n",
    "\n",
    "# path on the datastore where you will store the model object and data files\n",
    "iris_data_path = 'YOUR-INFO-HERE/.../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the compute target which should be provisioned already, and have at least 2 nodes\n",
    "COMPUTE_TARGET_NAME = 'YOUR-INFO-HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Get the model object and data </h2>\n",
    "I get data from the sklearn iris dataset, and the pretrained model object from:\n",
    "https://github.com/Azure-Samples/Machine-Learning-Operationalization/blob/master/samples/python/code/iris/model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpdym8_3se\n"
     ]
    }
   ],
   "source": [
    "# Get a temporary location for storage of downloaded items\n",
    "import tempfile\n",
    "iris_data_tmpdir = tempfile.mkdtemp()\n",
    "print(iris_data_tmpdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "924"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the model object from the Github repo\n",
    "\n",
    "import requests \n",
    "\n",
    "model_url = 'https://github.com/Azure-Samples/Machine-Learning-Operationalization/raw/master/samples/python/code/iris/model.pkl'\n",
    "r = requests.get(model_url, allow_redirects=True)\n",
    "model_data_local = os.path.join(iris_data_tmpdir, 'model.pkl')\n",
    "\n",
    "open(model_data_local, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "# Get the iris dataset, as CSV and Parquet\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "import pandas as pd\n",
    "iris_df = pd.DataFrame(data=iris['data'], columns = iris['feature_names'])\n",
    "print(len(iris_df))\n",
    "\n",
    "# Save as CSV\n",
    "iris_data_local_csv =  os.path.join(iris_data_tmpdir, 'iris.csv')\n",
    "iris_df.to_csv(iris_data_local_csv, sep = ',', index = False)\n",
    "\n",
    "# Save as Parquet\n",
    "iris_data_local_parquet =  os.path.join(iris_data_tmpdir, 'iris.parquet')\n",
    "iris_df.to_parquet(iris_data_local_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris.csv', 'iris.parquet', 'model.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list temporary directory contents\n",
    "os.listdir(iris_data_tmpdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create tablular datasets </h2>\n",
    "Move the iris data to blob storate and create datasets for both CSV and Parquet formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 2 files\n",
      "Uploading /tmp/tmpdym8_3se/iris.csv\n",
      "Uploaded /tmp/tmpdym8_3se/iris.csv, 1 files out of an estimated total of 2\n",
      "Uploading /tmp/tmpdym8_3se/iris.parquet\n",
      "Uploaded /tmp/tmpdym8_3se/iris.parquet, 2 files out of an estimated total of 2\n",
      "Uploaded 2 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_71b63019836e4276a9d9a6baabc98afa"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move files to blob storage\n",
    "iris_data.upload_files(files=[iris_data_local_csv, iris_data_local_parquet],\n",
    "                              target_path=iris_data_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "iris_ds_csv = Dataset.Tabular.from_delimited_files(path=[(iris_data,  \n",
    "                                                           '/'.join([iris_data_path, 'iris.csv']))], \n",
    "                                                           validate=False)\n",
    "iris_ds_parquet = Dataset.Tabular.from_parquet_files(path=[(iris_data,  \n",
    "                                                           '/'.join([iris_data_path, 'iris.parquet']))], \n",
    "                                                           validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Set output folder </h2>\n",
    "I use the same datastore for outputs as inputs.  You could put in your own information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "output_folder = PipelineData(name='inferences', datastore=iris_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Register model with workspace </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model iris-prs\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "# register downloaded model\n",
    "model = Model.register(model_path = model_data_local,\n",
    "                       model_name = \"iris-prs\", # this is the name the model is registered as\n",
    "                       tags = {'pretrained': \"iris\"},\n",
    "                       workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create experiment script </h2>\n",
    "I use this notebook to output a .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpjs3ml8_g\n"
     ]
    }
   ],
   "source": [
    "# Get a temporary location for storage of downloaded items\n",
    "import tempfile\n",
    "py_tmpdir = tempfile.mkdtemp()\n",
    "print(py_tmpdir)\n",
    "\n",
    "py_outfile = os.path.join(py_tmpdir, 'iris_score.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/tmpjs3ml8_g/iris_score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $py_outfile\n",
    "import io\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core.model import Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from azureml_user.parallel_run import EntryScript\n",
    "\n",
    "def init():\n",
    "    global iris_model\n",
    "\n",
    "    logger = EntryScript().logger\n",
    "    logger.info(\"init() is called.\")\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Iris model serving\")\n",
    "    parser.add_argument('--model_name', dest=\"model_name\", required=True)\n",
    "    args, unknown_args = parser.parse_known_args()\n",
    "\n",
    "    model_path = Model.get_model_path(args.model_name)\n",
    "    with open(model_path, 'rb') as model_file:\n",
    "        iris_model = pickle.load(model_file)\n",
    "\n",
    "\n",
    "def run(input_data):\n",
    "    logger = EntryScript().logger\n",
    "    logger.info(\"run() is called with: {}.\".format(input_data))\n",
    "\n",
    "    # make inference\n",
    "    num_rows, num_cols = input_data.shape\n",
    "    logger.info(\"num rows: {}.\".format(num_rows))\n",
    "    pred = iris_model.predict(input_data).reshape((num_rows, 1))\n",
    "\n",
    "    # cleanup output\n",
    "    result = input_data.drop(input_data.columns[4:], axis=1)\n",
    "    result['variety'] = pred\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_score.py']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(py_tmpdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Set up for batch run </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment\n",
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies\n",
    "\n",
    "predict_conda_deps = CondaDependencies.create(pip_packages=[\"scikit-learn==0.20.3\",\n",
    "                                                            \"azureml-core\", \"azureml-dataset-runtime[pandas,fuse]\"])\n",
    "\n",
    "predict_env = Environment(name=\"predict_environment\")\n",
    "predict_env.python.conda_dependencies = predict_conda_deps\n",
    "predict_env.docker.enabled = True\n",
    "predict_env.spark.precache_packages = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the parallel fun\n",
    "\n",
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "\n",
    "# In a real-world scenario, you'll want to shape your process per node and nodes to fit your problem domain.\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=py_tmpdir,\n",
    "    entry_script='iris_score.py',  # the user script to run against each input\n",
    "    mini_batch_size='1KB',\n",
    "    error_threshold=5,\n",
    "    output_action='append_row',\n",
    "    append_row_file_name=\"iris_outputs.txt\",\n",
    "    environment=predict_env,\n",
    "    compute_target=COMPUTE_TARGET_NAME, \n",
    "    node_count=2,\n",
    "    run_invocation_timeout=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Run the pipeline for the CSV input </h2>\n",
    "Note that this works fine, and uses 2-3 mini batches/processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline step\n",
    "distributed_csv_iris_step = ParallelRunStep(\n",
    "    name='example-iris-csv',\n",
    "    inputs=[iris_ds_csv.as_named_input('iris_data')],\n",
    "    output=output_folder,\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    arguments=['--model_name', 'iris-prs'],\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step example-iris-csv [3c691518][34ab4066-185e-40d9-b7cc-6470f9ce65fc], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun f3f94f2d-1084-42e6-b84b-ad04608f539b\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/iris-prs/runs/f3f94f2d-1084-42e6-b84b-ad04608f539b?wsid=/subscriptions/47a09743-a743-494e-945d-4022653e134e/resourcegroups/rg-flex-flightrisk-data-001/workspaces/mlw-flightrisk-dev\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[distributed_csv_iris_step])\n",
    "\n",
    "pipeline_run = Experiment(ws, 'iris-prs').submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: f3f94f2d-1084-42e6-b84b-ad04608f539b\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/iris-prs/runs/f3f94f2d-1084-42e6-b84b-ad04608f539b?wsid=/subscriptions/47a09743-a743-494e-945d-4022653e134e/resourcegroups/rg-flex-flightrisk-data-001/workspaces/mlw-flightrisk-dev\n",
      "{'runId': 'f3f94f2d-1084-42e6-b84b-ad04608f539b', 'status': 'Completed', 'startTimeUtc': '2021-02-10T16:07:51.00528Z', 'endTimeUtc': '2021-02-10T16:10:54.227702Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://mlwflightriskd8499840071.blob.core.windows.net/azureml/ExperimentRun/dcid.f3f94f2d-1084-42e6-b84b-ad04608f539b/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=29n5q%2BaHRjjuSm%2BtfyJVYcxgQBfQ11axoh2hgHpL1fw%3D&st=2021-02-10T15%3A59%3A42Z&se=2021-02-11T00%3A09%3A42Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://mlwflightriskd8499840071.blob.core.windows.net/azureml/ExperimentRun/dcid.f3f94f2d-1084-42e6-b84b-ad04608f539b/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=F%2FpUNjiiufh%2Boa8MnoNIKloCdSgnjlxWkr9YweNagT0%3D&st=2021-02-10T15%3A59%3A42Z&se=2021-02-11T00%3A09%3A42Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://mlwflightriskd8499840071.blob.core.windows.net/azureml/ExperimentRun/dcid.f3f94f2d-1084-42e6-b84b-ad04608f539b/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=vZyhkbonGkRDiYQnWT9blz5LXBIaKnNoseVsNQkv1H4%3D&st=2021-02-10T15%3A59%3A42Z&se=2021-02-11T00%3A09%3A42Z&sp=r'}, 'submittedBy': 'Valerie Anderson Carey'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Wait the run for completion \n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you can view the experiment and it should be successful.  Note that more than 1 mini batches are used!!  I get 3 mini batches\n",
    "The size of the CSV data on disk (for me) is 2.412 KiB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Now, try the parquet pipeline </h2>\n",
    "Note that this works but only uses 1 mini batch!  It doesn't matter what the batch size is, the Parquet file is always just 1 batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline step\n",
    "distributed_parquet_iris_step = ParallelRunStep(\n",
    "    name='example-iris-parquet',\n",
    "    inputs=[iris_ds_parquet.as_named_input('iris_data')],\n",
    "    output=output_folder,\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    arguments=['--model_name', 'iris-prs'],\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step example-iris-parquet [79769832][6f63348f-456b-4698-81fc-cf4a8775b953], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 2d13965f-4ee3-47d4-b64f-4a497e9c23cf\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/iris-prs/runs/2d13965f-4ee3-47d4-b64f-4a497e9c23cf?wsid=/subscriptions/47a09743-a743-494e-945d-4022653e134e/resourcegroups/rg-flex-flightrisk-data-001/workspaces/mlw-flightrisk-dev\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[distributed_parquet_iris_step])\n",
    "\n",
    "pipeline_run = Experiment(ws, 'iris-prs').submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 2d13965f-4ee3-47d4-b64f-4a497e9c23cf\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/iris-prs/runs/2d13965f-4ee3-47d4-b64f-4a497e9c23cf?wsid=/subscriptions/47a09743-a743-494e-945d-4022653e134e/resourcegroups/rg-flex-flightrisk-data-001/workspaces/mlw-flightrisk-dev\n",
      "{'runId': '2d13965f-4ee3-47d4-b64f-4a497e9c23cf', 'status': 'Completed', 'startTimeUtc': '2021-02-10T16:11:11.827901Z', 'endTimeUtc': '2021-02-10T16:13:45.428686Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://mlwflightriskd8499840071.blob.core.windows.net/azureml/ExperimentRun/dcid.2d13965f-4ee3-47d4-b64f-4a497e9c23cf/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=9Szyl9eu8UwOG%2BWwTwdPhwo8yyF20kBt2ieRMDycCmc%3D&st=2021-02-10T16%3A01%3A41Z&se=2021-02-11T00%3A11%3A41Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://mlwflightriskd8499840071.blob.core.windows.net/azureml/ExperimentRun/dcid.2d13965f-4ee3-47d4-b64f-4a497e9c23cf/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=8hBmy%2FdihNwwbSWzq088YYPL09JbDK9wa69QMAwkrVg%3D&st=2021-02-10T16%3A01%3A41Z&se=2021-02-11T00%3A11%3A41Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://mlwflightriskd8499840071.blob.core.windows.net/azureml/ExperimentRun/dcid.2d13965f-4ee3-47d4-b64f-4a497e9c23cf/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=gksJimmFMWL7NFv1KWvjzkRuKaCWK7FOL9qEcD3HRmA%3D&st=2021-02-10T16%3A01%3A41Z&se=2021-02-11T00%3A11%3A41Z&sp=r'}, 'submittedBy': 'Valerie Anderson Carey'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Wait the run for completion \n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for this run, you only get 1 mini batch.  The size of data on disk is 4.874 KiB (larger than CSV, and yet only 1 batch is used for Parquet as opposed to 3 with CSV!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "joringer"
   },
   {
    "name": "asraniwa"
   },
   {
    "name": "pansav"
   },
   {
    "name": "tracych"
   }
  ],
  "categories": [
   "how-to-use-azureml",
   "machine-learning-pipelines",
   "parallel-run"
  ],
  "category": "Other notebooks",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "IRIS"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "None"
  ],
  "friendly_name": "IRIS data inferencing using ParallelRunStep",
  "index_order": 1,
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "tags": [
   "Batch Inferencing",
   "Pipeline"
  ],
  "task": "Recognize flower type"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
